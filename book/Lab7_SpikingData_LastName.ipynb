{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab7: Spiking data processed by Nengo Neurons\n",
    "\n",
    "The objectives of this lab are:\n",
    "- To understand the format of event-based data\n",
    "- To undersatnd how event-based data is read by Nengo neurons\n",
    "  \n",
    "  \n",
    "> ### Specifications\n",
    "> ◻ Filename changed to reflect last name of person submitting assignment    \n",
    "> ◻ Code runs error free   \n",
    "> ◻ Jupyter notebook is saved such that outputs appear upon opening file (and therefore on gradescope)      \n",
    "> ◻ (On your honor) All markdown and resources read thoroughly and fully understood \n",
    "> ◻ AER is in proper format, either created by CSV or a python array  \n",
    "> ◻ Video generated from AER can be displayed outside of Nengo   \n",
    "> ◻ Video generated from AER can be read by Nengo neurons  \n",
    "> ◻ Video generated from AER can be displayed from Nengo simulation output spikes  \n",
    "> ◻ Video displayed from Nengo simulation output spikes can be clearly interpreted    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "**Ensure you are using your [495 Virtual Environment](https://github.com/kaitlin-fair/495venv_setup) before you begin!**  \n",
    "    \n",
    "Then, import Nengo, and other supporting libraries into your program to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "import nengo\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your spiking data (i.e. your AER)\n",
    "\n",
    "Your goal is to create an interesting spiking video with a *.csv file just like `ILAN City Frame Data.csv` OR with an event array (therefore bypassing the need for this line of code: `events = csv_to_event_array(csv_filename, start_time, end_time)`).  \n",
    "\n",
    "_Most interesting video gets a prize_ - have fun with this!\n",
    "\n",
    "**.csv file**\n",
    "\n",
    "Take a look at the `ILAN City Frame Data.csv` file. You will see in that file that you have x- and y-coordinates at which an event took place, a polarity of said event, and the time of said event. Your video must be at least 50x50 pixels and contain 10 frames (after you've read your data over windows of time). If you would like for your video to have both positive and negative spiking events, use polarities of 1 and 0, respectively. \n",
    "\n",
    "**events_array**\n",
    "\n",
    "The events array contains a list for every event consisting of the following values:   \n",
    "- `y`: The vertical coordinate of the event.\n",
    "- `x`: The horizontal coordinate of the event.\n",
    "- `p`: The polarity of the event (0 for off, 1 for on).\n",
    "- `t`: The event timestamp _in microseconds_. \n",
    "\n",
    "You can see the format of this list - `[(y, x, p, t)]` for each event - within the `csv_to_event_array` function. You then fill the final `events_array` with `np.array(events_list, dtype=[('x', 'i4'), ('y', 'i4'), ('p', 'i4'), ('t', 'i4')])`. \n",
    "\n",
    "**Notes**\n",
    "- You are welcome to add in negative polarity, but only if it's a trailing edge! Don't try to have \"color\" using negative polarity. Think of it more as a shadow to your positive polarity (leading edge).\n",
    "- You will see a spot to build a CSV or to build an array. CHOOSE ONE. Comment out / delete the other.\n",
    "- Do NOT mess with the `csv_to_event_array` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_event_array(csv_filename: str, start_frame: int, end_frame: int) -> np.ndarray:\n",
    "    # ======= DVS camera - Physics dept ===========\n",
    "    df = pd.read_csv(csv_filename, names=['x', 'y', 'p', 't'])\n",
    "    \n",
    "    # set initial time to 0 for Nengo simulator to run data right away\n",
    "    sub_df = df[(start_frame <= df['t']) & (df['t'] <= end_frame)]\n",
    "\n",
    "    sub_df['t'] = sub_df['t'] - sub_df['t'].iloc[0]\n",
    "\n",
    "    events_list = [(y, x, p, t) for x, y, p, t in sub_df.values]\n",
    "    events_array = np.array(events_list, dtype=[('y', 'i4'), ('x', 'i4'), ('p', 'i4'), ('t', 'i4')])\n",
    "\n",
    "    return events_array\n",
    "\n",
    "####################################### CHOOSE ONE ######################################\n",
    "\n",
    "#----------------Build a CSV--------------------\n",
    "\n",
    "# Your data! \n",
    "csv_filename = '???.csv'\n",
    "start_time = ???\n",
    "end_time = ???\n",
    "events = csv_to_event_array(csv_filename, start_time, end_time)\n",
    "print(\"Successfully read %r\" % csv_filename)\n",
    "\n",
    "#----------------Build an Array------------------\n",
    "\n",
    "# Your data!\n",
    "events_list =  ???\n",
    "               #[(x_loc1, y_loc1, polarity1, time event1 occurred),\n",
    "               # (x_loc2, y_loc2, polarity2, time event2 occurred),\n",
    "               # ...,\n",
    "               # (x_locN, y_locN, polarityN, time eventN occurred)]\n",
    "events = np.array(events_list, dtype=[('x', 'i4'), ('y', 'i4'), ('p', 'i4'), ('t', 'i4')])\n",
    "start_time = ???\n",
    "end_time = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data (NOT using neurons yet)\n",
    "\n",
    "You will need to adjust your image size. Depending on when you placed your events, you may need to adjust your dt_frame_us to determine the number of frames you will have in your video. For now, I'm leaving the time parameters as they were in our tutorial.\n",
    "\n",
    "This is your chance to ensure your video is doing what you thought it would! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Your Data! ===========\n",
    "img_height = ???\n",
    "img_width = ???\n",
    "t_length_us = end_time - start_time #value is in microseconds\n",
    "t_length_s = t_length_us * 1e-6 #convert prior value to seconds\n",
    "dt_frame_us = (10 * 1e-3) * 1e6 #this value is also in microseconds\n",
    "t_frames = dt_frame_us * np.arange(int(round(t_length_us / dt_frame_us))) #number of frames in video\n",
    "# ==============================\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0_us = t_frame\n",
    "    t1_us = t0_us + dt_frame_us\n",
    "    t = events[:][\"t\"]\n",
    "    m = (t >= t0_us) & (t < t1_us)\n",
    "    events_m = events[m]\n",
    "\n",
    "    # Empty frame\n",
    "    frame_img = np.zeros((img_height, img_width))\n",
    "\n",
    "    for sub_event in events_m:\n",
    "        # show \"off\" (0) events as -1 and \"on\" (1) events as +1\n",
    "        event_sign = 2.0 * sub_event[\"p\"] - 1\n",
    "        frame_img[sub_event[\"y\"], sub_event[\"x\"]] = frame_img[sub_event[\"y\"], sub_event[\"x\"]] + event_sign\n",
    "    \n",
    "    img = plt.imshow(frame_img[:, ::-1], vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Spike Data function to input into Nengo\n",
    "We can now load our data into a Nengo model using the `readSpikeData` function we will create. Recall, input arguments to this function are:\n",
    "- `data`:The path of the file to read from. Can be a .aedat or .events file. Format of the file will be detected from the file extension.\n",
    "- `pool`: Number of pixels to pool over in the vertical (first argument) and horizontal directions (second argument), respectively. The larger the pool, the fewer neurons required and the faster things run. **NOTE: image indices must be evenly divisible by pool size in respective directions.**\n",
    "- `img_height` and `image_width`: Dimensions of the camera data.\n",
    "\n",
    "Recall: we can have positive (leading edge) and negative (trailing edge) spikes from our camera called the polarity of the event. In this case we have 2: one for positive events, one for negative. The first half of our neurons represent positive events, the second half negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_DO NOT MESS WITH THE `read_spike_data` function!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a simple object to implement the delayed connection\n",
    "class readSpikeData:\n",
    "    def __init__(self, event_data, img_height, img_width, pool=(1,1)):\n",
    "        self.xvals = event_data[:][\"x\"]\n",
    "        self.yvals = event_data[:][\"y\"]\n",
    "        self.time = event_data[:][\"t\"] \n",
    "        self.pol = event_data[:][\"p\"]\n",
    "        self.img_ht = img_height\n",
    "        self.img_wt = img_width\n",
    "        self.pool = pool\n",
    "\n",
    "    def step(self, t):\n",
    "\n",
    "        dt = .001\n",
    "        t_lower = (t - dt) * 1e6\n",
    "        t_upper = t * 1e6\n",
    "\n",
    "        times = self.time\n",
    "        indices = np.nonzero((times >= t_lower) & (times < t_upper))[0]\n",
    "\n",
    "        pool_y, pool_x = self.pool\n",
    "        \n",
    "        data = np.zeros((self.img_ht*self.img_wt*2,), dtype=int)\n",
    "        for index in indices:\n",
    "            if self.pol[index] == 1:\n",
    "                # Manually flatten data using (i*y_len)+j **note x,y vals swapped\n",
    "                data[self.yvals[index]*self.img_wt + self.xvals[index]] = 1/dt \n",
    "            else:\n",
    "                data[self.img_ht*self.img_wt + self.yvals[index]*self.img_wt + self.xvals[index]] = 1/dt\n",
    "        \n",
    "        # reshape the data so pooling computations are more intuitive\n",
    "        data_sz = data.shape\n",
    "        pos_data = data[0:int(data_sz[0]/2)]\n",
    "        neg_data = data[int(data_sz[0]/2)::]\n",
    "        pos_data = pos_data.reshape(self.img_ht, self.img_wt)\n",
    "        neg_data = neg_data.reshape(self.img_ht, self.img_wt)\n",
    "\n",
    "        if pool_x > 1 or pool_y > 1:\n",
    "            pooled_ht = int(self.img_ht/pool_y)\n",
    "            pooled_wt = int(self.img_wt/pool_x)\n",
    "            pooled_posdata = np.zeros((pooled_ht,pooled_wt))\n",
    "            pooled_negdata = np.zeros((pooled_ht,pooled_wt))\n",
    "\n",
    "            for i in range(0, self.img_ht, pool_y):\n",
    "                for j in range(0, self.img_wt, pool_x):  \n",
    "                    pooled_posdata[int(i/pool_y),int(j/pool_x)] = np.mean(pos_data[i:i+pool_y, j:j+pool_x])\n",
    "                    pooled_negdata[int(i/pool_y),int(j/pool_x)] = np.mean(neg_data[i:i+pool_y, j:j+pool_x])\n",
    "            \n",
    "            pooled_posdata = pooled_posdata.reshape(pooled_ht* pooled_wt)\n",
    "            pooled_negdata = pooled_negdata.reshape(pooled_ht* pooled_wt)\n",
    "\n",
    "            pooled_data = np.append(pooled_posdata, pooled_negdata)\n",
    "            return pooled_data\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your model\n",
    "\n",
    "This should all look familiar! We are feeding in our input values (i.e. our spike data) through a node, representing the pixel changes with neurons, and reading the data out using probes.\n",
    "\n",
    "Notes: \n",
    "- You will likely not need to pool, but feel free to give it a go just to ensure you understand the pooling function. \n",
    "- Ensure your sim time accommodates all frames of your video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = (1, 1) #(pool_height y direction, pool_width x direction)\n",
    "    # NOTE // image indices must be evenly divisible by pool size in respective directions!\n",
    "\n",
    "inp = readSpikeData(events, img_height, img_width, pool)\n",
    "\n",
    "model = nengo.Network(label=\"Spiking Data\")\n",
    "with model:\n",
    "    input_node = nengo.Node(inp.step)\n",
    "\n",
    "    input_neurons = nengo.Ensemble(int(img_height/pool[0] * img_width/pool[1] * 2), 1)\n",
    "\n",
    "    nengo.Connection(input_node, input_neurons.neurons, transform=1.0)# / np.prod(pool))\n",
    "\n",
    "    probes_nodes = nengo.Probe(input_node)\n",
    "    probes = nengo.Probe(input_neurons.neurons)\n",
    "\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data (using Nengo neurons!)\n",
    "\n",
    "This section should not require any edits. **However, critically analyze your data! If I can't tell what's going on in this plot, you will earn a 0 in the lab. Why?** Keep reading.\n",
    "\n",
    "Well... if you see your images and you cannot actually see your events, remember that neurons are _noisy_. If your events are a single pixel in size, that will easily get lost in the noise. If this occurs, adjust your AER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_t = sim.trange()\n",
    "shape = (len(sim_t), int(img_height/pool[0]), int(img_width/pool[1]))\n",
    "\n",
    "output_spikes_pos = sim.data[probes][:,0:int(img_height/pool[0])*int(img_width/pool[1])].reshape(shape) * sim.dt\n",
    "output_spikes_neg = sim.data[probes][:,int(img_height/pool[0])*int(img_width/pool[1]):int(img_height/pool[0])*int(img_width/pool[1])*2].reshape(shape) * sim.dt\n",
    "\n",
    "dt_frame = dt_frame_us * 1e-6 # this is in seconds\n",
    "t_frames = dt_frame * np.arange(int(round(t_length_s / dt_frame)))\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0 = t_frame\n",
    "    t1 = t_frame + dt_frame\n",
    "    m = (sim_t >= t0) & (sim_t < t1)\n",
    "\n",
    "    frame_img = np.zeros((int(img_height/pool[0]), int(img_width/pool[1])))\n",
    "    frame_img -= output_spikes_neg[m].sum(axis=0)\n",
    "    frame_img += output_spikes_pos[m].sum(axis=0)\n",
    "    frame_img = frame_img / np.abs(frame_img).max()\n",
    "\n",
    "    img = plt.imshow(frame_img[:, ::-1], vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
