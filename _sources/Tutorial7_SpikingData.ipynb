{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_collect some interesting spiking data via prophosee camera, add in pooling_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking data on Nengo\n",
    "[Code cred](https://www.nengo.ai/nengo-examples/loihi/dvs-from-file.html)  \n",
    "This tutorial covers the following:\n",
    "- Format of spiking camera data\n",
    "- How to read spiking camera data\n",
    "- How to run spiking camera data through Nengo neurons\n",
    "***\n",
    "#### Motivation\n",
    "Many times we process spiking data into regular images, then perform image processing (such as detection and classification) using traditional CPUs. This is a valid way to do things!   \n",
    "\n",
    "BUT WHAT IF we instead feed the spiking data straight into neurons?? We can then just perform the image processing with neurons as well, further reducing power consumption and keeping the dimensionality of data reduced to events.  \n",
    " \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "If you are interested in using spiking data for your final project, take a look at a bunch of options [here](https://rpg.ifi.uzh.ch/davis_data.html), [here](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00405/full), and [here](https://docs.prophesee.ai/stable/datasets.html#chapter-datasets)\n",
    " -- or we can ask Dr. York to let us collect some data.\n",
    "\n",
    "[This website](https://rpg.ifi.uzh.ch/research_dvs.html) consolidates a lot of spiking-data resources.\n",
    "\n",
    "[This survey paper](https://rpg.ifi.uzh.ch/docs/EventVisionSurvey.pdf) discusses different event-based cameras available and how they are processed in sections 2.5 and 3.1 respectively.  \n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "**Ensure you are using your [495 Virtual Environment](https://kaitlin-fair.github.io/ECE495_Fa24/venv_setup.html) before you begin!**  \n",
    "    \n",
    "Then, import Nengo, and other supporting libraries into your program to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "import nengo\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your data (in the correct format)\n",
    "\n",
    "Hopefully you recall our discussion on event-based data earlier in the semester. If not, take a look back at Lecture 6.\n",
    "\n",
    "Hopefully you also recall building and utilizing Address Event Representation during our Path Planning lab. Combine those two concepts and you have spiking-camera data.\n",
    "\n",
    "We have spiking-camera data containing pixel-coordinates with a timestamp of an event occuring along with it's polarity (0 or 1) from the Physics department as well as from Dr. York. This data is stored in a *.csv file. We will read the *.csv file and convert it to an input array using `csv_to_event_array`, we will then use the array as our input using a Nengo node that calls `readSpikeData`.\n",
    "\n",
    "The *.csv files contain an array for every event consisting of the following values:   \n",
    "- `x`: The horizontal coordinate of the event.\n",
    "- `y`: The vertical coordinate of the event.\n",
    "- `p`: The polarity of the event (0 for off, 1 for on).\n",
    "- `t`: The event timestamp in microseconds.\n",
    "\n",
    "The `readSpikeData` function will send 1's as input at each time step at which there is an event. Each x-y location (i.e. pixel) will connect to a neuron. There will be both a positive and negative neuron for each pixel.\n",
    "\n",
    "**Other fun things about data**  \n",
    "We can also generate data if we don't have our own (copy code from [here](https://www.nengo.ai/nengo-examples/loihi/dvs-from-file.html)). AND there are even Github repositories [like this one](https://github.com/SensorsINI/v2e) out there where you can take any data you'd like and convert it to spiking data like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_event_array(csv_filename: str, start_frame: int, end_frame: int) -> np.ndarray:\n",
    "    # ======= DVS camera - Physics dept ===========\n",
    "    df = pd.read_csv(csv_filename, names=['x', 'y', 'p', 't'])\n",
    "    \n",
    "    # set initial time to 0 for Nengo simulator to run data right away\n",
    "    sub_df = df[(start_frame <= df['t']) & (df['t'] <= end_frame)]\n",
    "\n",
    "    sub_df['t'] = sub_df['t'] - sub_df['t'].iloc[0]\n",
    "\n",
    "    events_list = [(y, x, p, t) for x, y, p, t in sub_df.values]\n",
    "    events_array = np.array(events_list, dtype=[('y', 'i4'), ('x', 'i4'), ('p', 'i4'), ('t', 'i4')])\n",
    "\n",
    "    return events_array\n",
    "\n",
    "# the csv file we will convert and the time range of our events\n",
    "\n",
    "# ======= DVS camera - Physics dept ===========\n",
    "csv_filename = 'ILAN City Frame Data.csv'\n",
    "start_time = 585861\n",
    "end_time = 685846\n",
    "# =============================================\n",
    "\n",
    "# ======== Prophesee camera - ECE dept ========\n",
    "# csv_filename = 'prophesee.csv'\n",
    "# start_time = 55e5\n",
    "# end_time = 7e6\n",
    "# =============================================\n",
    "\n",
    "# Our events array we will feed in through a Nengo node\n",
    "events = csv_to_event_array(csv_filename, start_time, end_time)\n",
    "print(\"Successfully read %r\" % csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data (NOT using neurons yet)\n",
    "\n",
    "This is all just a lot of python-ness to display a video. _Big takeaway_ - we are adding up events over a window of time. Our events occur over the course of 100ms. We take 10ms chunks and set any pixels within that 100ms with an event to 1 (or -1 should we have a polarity of 0). This gives us 10 frames.\n",
    "\n",
    "You can read all about animations [here](https://matplotlib.org/stable/users/explain/animations/animations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Prophesee camera - ECE dept ========\n",
    "# img_height = 720\n",
    "# img_width = 1280\n",
    "# t_length_us = end_time - start_time #value is in microseconds\n",
    "# t_length_s = t_length_us * 1e-6 #convert prior value to seconds\n",
    "# dt_frame_us = (10 * 1e-3) * 1e6 #this value is also in microseconds\n",
    "# t_frames = dt_frame_us * np.arange(int(round(t_length_us / dt_frame_us))) #number of frames in video\n",
    "# =============================================\n",
    "\n",
    "# ======= DVS camera - Physics dept ===========\n",
    "img_height = 180\n",
    "img_width = 240\n",
    "t_length_us = end_time - start_time #value is in microseconds\n",
    "t_length_s = t_length_us * 1e-6 #convert prior value to seconds\n",
    "dt_frame_us = (10 * 1e-3) * 1e6 #this value is also in microseconds\n",
    "t_frames = dt_frame_us * np.arange(int(round(t_length_us / dt_frame_us))) #number of frames in video\n",
    "# =============================================\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0_us = t_frame\n",
    "    t1_us = t0_us + dt_frame_us\n",
    "    t = events[:][\"t\"]\n",
    "    m = (t >= t0_us) & (t < t1_us)\n",
    "    events_m = events[m]\n",
    "\n",
    "    # Empty frame\n",
    "    frame_img = np.zeros((img_height, img_width))\n",
    "\n",
    "    for sub_event in events_m:\n",
    "        # show \"off\" (0) events as -1 and \"on\" (1) events as +1\n",
    "        event_sign = 2.0 * sub_event[\"p\"] - 1\n",
    "        frame_img[sub_event[\"y\"], sub_event[\"x\"]] = frame_img[sub_event[\"y\"], sub_event[\"x\"]] + event_sign\n",
    "    \n",
    "    img = plt.imshow(frame_img[:, ::-1], vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Spike Data function to input into Nengo\n",
    "We can now load our data into a Nengo model using the `readSpikeData` function we will create. Input arguments to this function are:\n",
    "- `data`:The path of the file to read from. Can be a .aedat or .events file. Format of the file will be detected from the file extension.\n",
    "- `pool`: Number of pixels to pool over in the vertical and horizontal directions, respectively. The larger the pool, the fewer neurons required. **Currently not working - must use [1 1].**\n",
    "- `img_height` and `image_width`: Dimensions of the camera data.\n",
    "\n",
    "Note that we can have positive (leading edge) and negative (trailing edge) spikes from our camera called the polarity of the event. In this case we have 2: one for positive events, one for negative. The first half of our neurons represent positive events, the second half negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a simple object to implement the delayed connection\n",
    "class readSpikeData:\n",
    "    def __init__(self, event_data, img_height, img_width, pool=(1,1)):\n",
    "        self.xvals = event_data[:][\"x\"]\n",
    "        self.yvals = event_data[:][\"y\"]\n",
    "        self.time = event_data[:][\"t\"] \n",
    "        self.pol = event_data[:][\"p\"]\n",
    "        self.img_ht = img_height\n",
    "        self.img_wt = img_width\n",
    "        self.pool = pool\n",
    "\n",
    "    def step(self, t):\n",
    "\n",
    "        dt = .001\n",
    "        t_lower = (t - dt) * 1e6\n",
    "        t_upper = t * 1e6\n",
    "\n",
    "        times = self.time\n",
    "        indices = np.nonzero((times >= t_lower) & (times < t_upper))[0]\n",
    "\n",
    "        pool_y, pool_x = self.pool\n",
    "        \n",
    "        data = np.zeros((self.img_ht*self.img_wt*2,), dtype=int)\n",
    "        for index in indices:\n",
    "            if self.pol[index] == 1:\n",
    "                # Manually flatten data using (i*y_len)+j **note x,y vals swapped\n",
    "                data[self.yvals[index]*self.img_wt + self.xvals[index]] = 1/dt \n",
    "            else:\n",
    "                data[self.img_ht*self.img_wt + self.yvals[index]*self.img_wt + self.xvals[index]] = 1/dt\n",
    "        \n",
    "        # reshape the data so pooling computations are more intuitive\n",
    "        data_sz = data.shape\n",
    "        pos_data = data[0:int(data_sz[0]/2)]\n",
    "        neg_data = data[int(data_sz[0]/2)::]\n",
    "        pos_data = pos_data.reshape(self.img_ht, self.img_wt)\n",
    "        neg_data = neg_data.reshape(self.img_ht, self.img_wt)\n",
    "\n",
    "        if pool_x > 1 or pool_y > 1:\n",
    "            pooled_ht = self.img_ht/pool_y\n",
    "            pooled_wt = self.img_wt/pool_x\n",
    "            pooled_data = np.zeros((pooled_ht*pooled_wt*2,), dtype=int)\n",
    "\n",
    "            pooled_data = sum(data(pool_idx)) / (pool_x*pool_y)\n",
    "            \n",
    "            return pooled_data\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your model\n",
    "\n",
    "This should all look familiar! We are feeding in our input values (i.e. our spike data) through a node, representing the pixel changes with neurons, and reading the data out using probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= DVS camera - Physics dept ===========\n",
    "pool = (1, 1)\n",
    "\n",
    "# ======== Prophesee camera - ECE dept ========\n",
    "# pool = (10, 10)\n",
    "\n",
    "inp = readSpikeData(events, img_height, img_width, pool)\n",
    "\n",
    "model = nengo.Network(label=\"Spiking Data\")\n",
    "with model:\n",
    "    input_node = nengo.Node(inp.step)\n",
    "\n",
    "    input_neurons = nengo.Ensemble(img_height * img_width * 2, 1)\n",
    "\n",
    "    nengo.Connection(input_node, input_neurons.neurons, transform=1.0)# / np.prod(pool))\n",
    "\n",
    "    probes_nodes = nengo.Probe(input_node)\n",
    "    probes = nengo.Probe(input_neurons.neurons)\n",
    "\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(t_length_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data (using Nengo neurons!)\n",
    "\n",
    "This looks very much the same as viewing the data with regular Python - except now we are reading spikes from our probed neurons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_t = sim.trange()\n",
    "shape = (len(sim_t), img_height, img_width)\n",
    "\n",
    "output_spikes_pos = sim.data[probes][:,0:img_height*img_width].reshape(shape) * sim.dt\n",
    "output_spikes_neg = sim.data[probes][:,img_height*img_width:img_height*img_width*2].reshape(shape) * sim.dt\n",
    "\n",
    "dt_frame = dt_frame_us * 1e-6 # this is in seconds\n",
    "t_frames = dt_frame * np.arange(int(round(t_length_s / dt_frame)))\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0 = t_frame\n",
    "    t1 = t_frame + dt_frame\n",
    "    m = (sim_t >= t0) & (sim_t < t1)\n",
    "\n",
    "    frame_img = np.zeros((img_height, img_width))\n",
    "    frame_img -= output_spikes_neg[m].sum(axis=0)\n",
    "    frame_img += output_spikes_pos[m].sum(axis=0)\n",
    "    frame_img = frame_img / np.abs(frame_img).max()\n",
    "\n",
    "    img = plt.imshow(frame_img[:, ::-1], vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Notice the noise is HIGH. That's because neurons are inherently noisy. \n",
    "\n",
    "**_So again, why did we do this?_**   \n",
    "\n",
    "It seemed like a lot of work to just print the data again... right??\n",
    "\n",
    "Well, now we know we can effectively send the data through neurons, which means we can now start processing the data with neurons and maintain the efficiencies we gained by collecting event-based data in the first place. COOL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-nengo3.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
