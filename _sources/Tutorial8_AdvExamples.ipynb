{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_add in read data stuff developed from final project q's. mostly contained in data processing folder. get rid of delay node (taught that in path planning). matrix multiplication example is kind of meh. Hit hard on reading data and online learning. possibly make the lab a combo of those two._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Examples\n",
    "\n",
    "In this demonstration we will:\n",
    "\n",
    "1. Perform matrix multiplication with greater than 2-dimensional neurons\n",
    "2. Create a delay using nodes\n",
    "3. Learn decoders in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "\n",
    "**Ensure you are using your [495 Virtual Environment](https://github.com/kaitlin-fair/495venv_setup) before you begin!**  \n",
    "    \n",
    "Import Nengo and other supporting libraries into your program to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nengo\n",
    "from nengo.processes import Piecewise\n",
    "from nengo.processes import WhiteSignal\n",
    "from nengo.solvers import LstsqL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "This example demonstrates how to perform general matrix multiplication using Nengo. The matrix can change during the computation, which makes it distinct from doing static matrix multiplication with neural connection weights (as done in all neural networks).\n",
    "\n",
    "Let's compute $A \\cdot B$, which is equal to $(B \\cdot A)^T$ where\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "\t\t.5 & -.5 \\\\\n",
    "\t\t-.2 & .3  \n",
    "\t\\end{bmatrix}$  and  $B = \t\\begin{bmatrix}\n",
    "\t\t\t\t\t\t\t\t\t.58 & -1 \\\\\n",
    "\t\t\t\t\t\t\t\t\t.7 & .1  \n",
    "\t\t\t\t\t\t\t\t\\end{bmatrix}$\n",
    "\n",
    "Big differences here:\n",
    "- We are using an _EnsembleArray_ (i.e. an array of ensembles of neurons) instead of an _Ensemble_ of neurons\n",
    "- We specifically _probe the outputs of an Ensemble Array_ since we need to probe each ensemble's output (not the array itself). This results in 4 outputs since we have an array of 4 ensembles of neurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amat = np.array([[0.5, -0.5], [-0.2, 0.3]])\n",
    "Bmat = np.array([[0.58, -1.0], [0.7, 0.1]])\n",
    "\n",
    "model = nengo.Network(label=\"Matrix Multiplication\", seed=123)\n",
    "with model:\n",
    "    # Make 2 EnsembleArrays to encode the input\n",
    "    # Amat.size and Bmat.size = 4, therefore 4D neurons used\n",
    "    # Values should stay within the range (-radius, radius)\n",
    "    A = nengo.networks.EnsembleArray(n_neurons = 100, n_ensembles=Amat.size, radius=1) \n",
    "    B = nengo.networks.EnsembleArray(n_neurons = 100, n_ensembles=Bmat.size, radius=1)\n",
    "\n",
    "    # Create 2 nodes to send in matrix values\n",
    "    inputA = nengo.Node(Amat.ravel())  # ravel is equivalent to reshape(-1) i.e. flatten!\n",
    "    inputB = nengo.Node(Bmat.ravel())\n",
    "\n",
    "    # connect input nodes to neuron ensembles\n",
    "    nengo.Connection(inputA, A.input)\n",
    "    nengo.Connection(inputB, B.input)\n",
    "\n",
    "    # Probe to check we've connected correctly\n",
    "    #  Note: must probe the output of each ensemble because \n",
    "    #        we are using an array of ensembles!\n",
    "    #\n",
    "    # sample_every determines how many timesteps you plot\n",
    "    #  in this case, it's fewer times than the simulation would plot\n",
    "    # default simulation dt = .001\n",
    "    A_probe = nengo.Probe(A.output, sample_every=0.01, synapse=0.01) \n",
    "    B_probe = nengo.Probe(B.output, sample_every=0.01, synapse=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(1)\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"A\")\n",
    "plt.plot(sim.trange(sample_every=0.01), sim.data[A_probe])\n",
    "plt.legend(['1','2','3','4'], loc=\"best\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"B\")\n",
    "plt.plot(sim.trange(sample_every=0.01), sim.data[B_probe])\n",
    "plt.legend(['1','2','3','4'], loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs look good, but now we need to actually perform each multiplication that occurs in matrix multiplication. \n",
    "\n",
    "First, we will use one of Nengo's reusable networks `product` instead of manually creating multiplication neurons as we did in `neuron_transformations.ipynb` (although we could definitely do it this way!). The `product` network computes the element-wise product of two equally sized vectors where `input_a` is the first vector and `input_b` is the second. Read more about different reusable networks [here](https://www.nengo.ai/nengo/networks.html#nengo.networks.Product).\n",
    "\n",
    "You'll notice we call an argument `transform`. It's way easier to use transform than a function when it comes to matrix multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # The C matrix is composed of populations that each contain\n",
    "    # one element of A and one element of B.\n",
    "    # These elements will be multiplied together in the next step.\n",
    "\n",
    "    # For two 2x2 matrices to be multiplied together, there will be \n",
    "    #  8 total multiplications that will occur (think through this!)\n",
    "    c_size = Amat.size * Bmat.shape[1] # 4*2 = 8\n",
    "    C = nengo.networks.Product(100, dimensions=c_size)\n",
    "\n",
    "# Determine the transformation matrices to get the correct pairwise\n",
    "#   products computed. \n",
    "transformA = np.zeros((c_size, Amat.size))\n",
    "transformB = np.zeros((c_size, Bmat.size))\n",
    "\n",
    "for i in range(Amat.shape[0]):\n",
    "    for j in range(Amat.shape[1]):\n",
    "        for k in range(Bmat.shape[1]):\n",
    "            c_index = j + k * Amat.shape[1] + i * Bmat.size\n",
    "            transformA[c_index][j + i * Amat.shape[1]] = 1\n",
    "            transformB[c_index][k + j * Bmat.shape[1]] = 1\n",
    "\n",
    "print(\"A->C\")\n",
    "print(transformA)\n",
    "print(\"B->C\")\n",
    "print(transformB)\n",
    "\n",
    "with model:\n",
    "    conn = nengo.Connection(A.output, C.input_a, transform=transformA)\n",
    "    nengo.Connection(B.output, C.input_b, transform=transformB)\n",
    "    C_probe = nengo.Probe(C.output, sample_every=0.01, synapse=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed the weights. Recall, we have two 2x2 matrices. Each matrix was flattened into an array of 4 values (indices 0 to 3). If there is a `1` in the tranform matrix, that means the element is \"active\" because it is weighted 1 instead of 0. So, for the first multiplication that must be performed, we have $a_0$ multiplied by $b_0$.\n",
    "\n",
    "If you take a look at each transform matrix row by row, they line up to perform each multiplication required within two 2x2 matrices being multiplied. \n",
    "\n",
    "$$ \\vec{A} \\vec{B}  \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{11} & a_{12} \\\\\n",
    "\ta_{21} & a_{22}  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\tb_{11} & b_{12} \\\\\n",
    "\tb_{21} & b_{22}  \t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{0} & a_{1} \\\\\n",
    "\ta_{2} & a_{3}  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\tb_{0} & b_{1} \\\\\n",
    "\tb_{2} & b_{3}  \t\n",
    "\\end{bmatrix}\n",
    "\\\\ \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{11}  b_{11} + a_{12}b_{21} & a_{11} b_{12}+a_{12}b_{22} \\\\\n",
    "\ta_{21}  b_{11} + a_{22}b_{21} & a_{21}  b_{12} + a_{22}b_{22} \t\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{0}  b_{0} + a_{1}b_{2} & a_{0} b_{1}+a_{1}b_{3} \\\\\n",
    "\ta_{2}  b_{0} + a_{3}b_{2} & a_{2}  b_{1} + a_{0}b_{3} \t\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Now, let's view the outputs of our 8-dimensional neuron ensemble C (created from a pre-built Nengo network `product`). This will output each of the individual multiplications. Note: no addition has occurred!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it!\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(1)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plt.plot(sim.trange(sample_every=0.01), sim.data[C_probe])\n",
    "plt.title(\"C\")\n",
    "plt.legend(['1','2','3','4','5','6','7','8'], loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to add the appropriate elements to complete matrix multiplication. We use the same `transform` argument (i.e. decoders/weights) to choose which of the outputs from C should be added together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # Now do the appropriate summing\n",
    "    D = nengo.networks.EnsembleArray(\n",
    "        100, n_ensembles=Amat.shape[0] * Bmat.shape[1], radius=1\n",
    "    ) # sanity check that this is the right size for any matrix multiplication!\n",
    "\n",
    "# The mapping for this transformation is much easier, since we want to\n",
    "# combine pairs from C within D.\n",
    "transformC = np.zeros((D.dimensions, c_size))\n",
    "for i in range(c_size):\n",
    "    transformC[i // Bmat.shape[0]][i] = 1\n",
    "print(\"C->D\")\n",
    "print(transformC)\n",
    "\n",
    "with model:\n",
    "    nengo.Connection(C.output, D.input, transform=transformC)\n",
    "    D_probe = nengo.Probe(D.output, sample_every=0.01, synapse=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 8 outputs from C. We add two at a time to compute the four outputs of our two 2x2 matrices being multiplied together.\n",
    "\n",
    "$$ \\vec{A} \\vec{B}  \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{11} & a_{12} \\\\\n",
    "\ta_{21} & a_{22}  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\tb_{11} & b_{12} \\\\\n",
    "\tb_{21} & b_{22}  \t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{0} & a_{1} \\\\\n",
    "\ta_{2} & a_{3}  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\tb_{0} & b_{1} \\\\\n",
    "\tb_{2} & b_{3}  \t\n",
    "\\end{bmatrix}\n",
    "\\\\ \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{11}  b_{11} + a_{12}b_{21} & a_{11} b_{12}+a_{12}b_{22} \\\\\n",
    "\ta_{21}  b_{11} + a_{22}b_{21} & a_{21}  b_{12} + a_{22}b_{22} \t\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "\ta_{0}  b_{0} + a_{1}b_{2} & a_{0} b_{1}+a_{1}b_{3} \\\\\n",
    "\ta_{2}  b_{0} + a_{3}b_{2} & a_{2}  b_{1} + a_{0}b_{3} \t\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Now, let's run it and see our final answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it!\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(1)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plt.plot(sim.trange(sample_every=0.01), sim.data[D_probe])\n",
    "for d in np.dot(Amat, Bmat).flatten():\n",
    "    plt.axhline(d, color=\"k\")\n",
    "plt.title(\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "We could have written matrix multiplication manually using our own multiplication function from last time. That may have made it more intuitive, and we could have possibly found a faster way to do it! \n",
    "\n",
    "BUT, now you have experience using a pre-built Nengo network and a generalized form of matrix multiplication, which may come in handy for your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delays using Nengo Nodes\n",
    "\n",
    "We can use a Nengo node to implement an n-timestep delayed connection by using a node. Why might you want a delay? Recall our bubblesort outputs. They were not precise as we had expected because of some of the feedback delays due to calculations. We may want to send something in at the exact same time to prevent this (or explore dynamics to adjust for the delays - both options have pros/cons!). Ultimately, you may want to wait to send an input into a neuron for a variety of reasons and this code will help you make that happen.\n",
    "\n",
    "This example uses object-oriented programming to set up the delay. You can find a pretty good tutorial on the topic [here](https://www.udacity.com/blog/2021/11/__init__-in-python-an-overview.html). You can also see how `np.roll` works [here](https://numpy.org/doc/stable/reference/generated/numpy.roll.html).\n",
    "\n",
    "The class `Delay` starts off with an array called `self.history` of 250 zeros (each time step is `dt=.001` seconds and we want a `time_delay=25` seconds, which is in total 250 time steps). For the first actual time step of the system ($t=1$), the `self.history` array updates to $\\begin{bmatrix} 0 & 0 & ... & x(1) \\end{bmatrix}$, but what leaves the node is the first element of the array, 0. This is what we want! We are delaying our signal while storing what will come 250 timesteps later. At the next time step ($t=2$), the `self.history` array updates to $\\begin{bmatrix} 0 & 0 & ... & x(1) & x(2) \\end{bmatrix}$. Keep going until time step $t=250$ and you should get the `self.history` array $\\begin{bmatrix} x(1) & x(2) & ... & x(249) & x(250) \\end{bmatrix}$ and the first element $x(1)$ is what leaves the node. That means at time step 250, you're just now seeing the start of the original signal, i.e. you've delayed the signal by 250 timesteps!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nengo.Network(label=\"Delayed connection\")\n",
    "with model:\n",
    "    # We'll use white noise as input\n",
    "    inp = nengo.Node(WhiteSignal(2, high=5), size_out=1)\n",
    "    A = nengo.Ensemble(40, dimensions=1)\n",
    "    nengo.Connection(inp, A)\n",
    "\n",
    "\n",
    "# We'll make a simple object to implement the delayed connection\n",
    "class Delay:\n",
    "    def __init__(self, dimensions, timesteps=50):\n",
    "        self.history = np.zeros((timesteps, dimensions))\n",
    "\n",
    "    def step(self, t, x):\n",
    "        self.history = np.roll(self.history, -1)\n",
    "        self.history[-1] = x\n",
    "        return self.history[0]\n",
    "\n",
    "dt = 0.001 # default nengo timestep\n",
    "time_delay = .25 # seconds by which you wish to delay the signal\n",
    "delay = Delay(1, timesteps=int(time_delay / dt))\n",
    "\n",
    "with model:\n",
    "    delaynode = nengo.Node(delay.step, size_in=1, size_out=1)\n",
    "    nengo.Connection(A, delaynode)\n",
    "\n",
    "    # Send the delayed output through an ensemble\n",
    "    B = nengo.Ensemble(40, dimensions=1)\n",
    "    nengo.Connection(delaynode, B)\n",
    "\n",
    "    # Probe the input at the delayed output\n",
    "    A_probe = nengo.Probe(A, synapse=0.01)\n",
    "    B_probe = nengo.Probe(B, synapse=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 2 seconds\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(2)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(sim.trange(), sim.data[A_probe], lw=2)\n",
    "plt.title(\"Input\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(sim.trange(), sim.data[B_probe], lw=2)\n",
    "plt.axvline(time_delay, c=\"k\")\n",
    "plt.title(\"Delayed output\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Learning\n",
    "\n",
    "Normally, if you have a function you would like to compute across a connection, you would specify it with `function` in the Connection constructor. However, it is also possible to use error-driven learning to learn to compute a function online (i.e. while you're actively receiving inputs). This is relevant to _edge computing_ which we discussed quite a bit in our Neuroscience and Neural Nets lectures. Suppose you've trained a DNN to classify certain data, but now you get new data while in the field and want to update your network in real-time. This is NOT an easy task with current state of the art - in fact a lot of dollars are going in to trying to solve this problem. Our brains however have already solved it. These neurons are inspired by our brains. So let's learn in real time with these neurons.\n",
    "\n",
    "First, here's our model without learning - i.e. Nengo computes all of the weights for the connections in advance. In this case, we have two ensembles acting as a communication channel (we built a simple version of this in `neuron_transformations.ipynb`). In a comms channel, you want the output of the first ensemble fed into the second, and you want the second ensemble's output to match that of the original input.\n",
    "\n",
    "[Input] ---> (A) ---> (B) ---> [Output]\n",
    "\n",
    "To demonstrate online learning, we add in a function between the two ensembles (i.e. the output of the second ensemble is a function of the first - which is why we have weights, to compute that f(x)). NOTE: in a comms channel, our f(x) = x. BUT we want to mess that up so we have to relearn the weights to make that happen. SO, we add a weird function in between that we will train away by the end of this demo. \n",
    "\n",
    "Notice both ensembles are 2D - meaning we send two white noise signals into the two inputs of the first ensemble, and we get a white noise signal out of the two output dimensions. THEN, we compute a bizarre function where we just return two random numbers out of the second ensemble. To recap: we send two white noise signals into Ensemble A, connect to Ensemble B, which then outputs two random values from the network using pretrained Nengo decoders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nengo.Network(label=\"Pretrained Comms Channel\")\n",
    "with model:\n",
    "    # input signal, passed from node to ensemble A\n",
    "    input = nengo.Node(WhiteSignal(60, high=5), size_out=2)\n",
    "    A = nengo.Ensemble(60, dimensions=2)\n",
    "    nengo.Connection(input, A)\n",
    "\n",
    "    # passed output of ensemble A to ensemble B \n",
    "    # train weights with a complicated function (send in a value, return something random)\n",
    "    B = nengo.Ensemble(60, dimensions=2)\n",
    "    conn = nengo.Connection(A, B, function=lambda x: np.random.random(2))\n",
    "    \n",
    "    inp_p = nengo.Probe(input)\n",
    "    pre_p = nengo.Probe(A, synapse=0.01)\n",
    "    post_p = nengo.Probe(B, synapse=0.01)\n",
    "\n",
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(10.0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(sim.trange(), sim.data[inp_p].T[0], c=\"k\", label=\"Input\")\n",
    "plt.plot(sim.trange(), sim.data[pre_p].T[0], c=\"b\", label=\"Pre\")\n",
    "plt.plot(sim.trange(), sim.data[post_p].T[0], c=\"r\", label=\"Post\")\n",
    "plt.ylabel(\"Dimension 1\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(sim.trange(), sim.data[inp_p].T[1], c=\"k\", label=\"Input\")\n",
    "plt.plot(sim.trange(), sim.data[pre_p].T[1], c=\"b\", label=\"Pre\")\n",
    "plt.plot(sim.trange(), sim.data[post_p].T[1], c=\"r\", label=\"Post\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in Learning\n",
    "\n",
    "Now, we will add an ensemble to compute our error. We know that with a proper comms channel, our output from the ensemble B should match that of ensemble A. The error is therefore the difference between the two and we will use an ensemble to compute that. The error neuron will subtract ensemble A output from ensemble B output using the `transform=-1` argument for the A connection(i.e. error $= B-A =$ how far off we are from where we want to be).\n",
    "\n",
    "We will then employ an error rule on our connection between A and B, which we called `conn` in the last segment of code. The learning rule we use is called the [prescribed error sensitivity (PES) learning rule](https://www.researchgate.net/publication/282366687_A_Solution_to_the_Dynamics_of_the_Prescribed_Error_Sensitivity_Learning_Rule). The PES learning rule is a biologically plausible supervised learning rule that is frequently used with the Neural Engineering Framework (NEF) -- this is Nengo's framework! PES modifies the connection weights between populations of neurons to minimize an external error signal (being the error ensemble we've created). [This page](https://www.nengo.ai/nengo/examples/learning/learn-communication-channel.html) describes it succinctly in the \"How does this work?\" section if you want the math behind how the decoders are updated. \n",
    "\n",
    "This means we compute an error and use that in our learning rule between ensembles A and B to push the weights toward a solution where $B-A=0$, i.e. $A=B$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    error = nengo.Ensemble(60, dimensions=2)\n",
    "    error_probe = nengo.Probe(error, synapse=0.03)\n",
    "\n",
    "    # Error = actual - target = post - pre\n",
    "    nengo.Connection(B, error)\n",
    "    connA = nengo.Connection(A, error, transform=-1)\n",
    "\n",
    "    # Add the learning rule to the connection\n",
    "    conn.learning_rule_type = nengo.PES()\n",
    "\n",
    "    # Connect the error into the learning rule\n",
    "    nengo.Connection(error, conn.learning_rule)\n",
    "\n",
    "    # probe the decoders to see how those change over time to reduce error\n",
    "    weights_p = nengo.Probe(conn, \"weights\", synapse=0.01, sample_every=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it, plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(10.0)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(sim.trange(), sim.data[inp_p].T[0], c=\"k\", label=\"Input\")\n",
    "plt.plot(sim.trange(), sim.data[pre_p].T[0], c=\"b\", label=\"Pre\")\n",
    "plt.plot(sim.trange(), sim.data[post_p].T[0], c=\"r\", label=\"Post\")\n",
    "plt.ylabel(\"Dimension 1\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(sim.trange(), sim.data[inp_p].T[1], c=\"k\", label=\"Input\")\n",
    "plt.plot(sim.trange(), sim.data[pre_p].T[1], c=\"b\", label=\"Pre\")\n",
    "plt.plot(sim.trange(), sim.data[post_p].T[1], c=\"r\", label=\"Post\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(sim.trange(), sim.data[error_probe], c=\"b\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend((\"Error[0]\", \"Error[1]\"), loc=\"best\")\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(sim.trange(sample_every=0.01), sim.data[weights_p][..., 10])\n",
    "plt.ylabel(\"Decoding weight\")\n",
    "plt.legend((\"Decoder 10[0]\", \"Decoder 10[1]\"), loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "We can see here that toward the end of our simulation, both dimensions start to look the same. We can see from the third plot that error is reduced as we go in each dimension. Once error is reduced to near zero, the weights should always work, regardless of input to the network (i.e. if we keep those decoders between those two ensembles, we can feed in any input and we should output the same signal). How? Decoders! You can see in the fourth plot how the decoders change and then converge to a value over time. That means that instead of computing that nasty random output function, the weights have adjusted to take the same output spikes and compute $f(x)=x$. If you're lost, please work back through the `neuron_transformations.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-nengo3.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
